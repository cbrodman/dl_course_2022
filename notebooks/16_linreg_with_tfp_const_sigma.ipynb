{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16_linreg_with_tfp_const_sigma.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jNcCptvK1Sj9"
      },
      "source": [
        "# Modelling continuous data with Tensoflow Probability\n",
        "\n",
        "In this notebook you will learn how work with TFP. You will set up linear regression models that are able to output a gaussian conditional probability distribution. You will define different models with Keras and the Tensorflow probability framework and optimize the negative log likelihood (NLL). You will model the conditional probability distribution as a Normal distribution with a constant standart deviation $\\sigma$. The mean $\\mu$ of the CPD will always depend linearly on the input. You will model the constant $\\sigma$ by hand with the formula and Tensorflow probability and compare the optimal $\\sigma$ from the formula with the solution of TFP.\n",
        "\n",
        "**Dataset:** \n",
        "You work with a simulated dataset that looks a bit like a fish when visualized in a scatterplot. The data is simulated to have  a linear slope but non-constant variance. The variance starts with a large value then slowly decreases to increase again and in the end it decreases again. You split the data into train, validation and test dataset.\n",
        "\n",
        "**Content:**\n",
        "* Simulate and split the dataset \n",
        "* Fit a model with keras and TFP that models the CPD with a linear mean $\\mu$ and a fixed standart deviation $\\sigma$ by hand.\n",
        "* Fit a model with keras and TFP that models the CPD with a linear mean $\\mu$ and a fixed standart deviation $\\sigma$ with TFP.\n",
        "* Compare the two models based on the NLL loss and the optimal $\\sigma$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9mojpXQ4Rj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t4IJDOa3Qkni",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow_probability==0.8.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6_m-OXstXNm"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QllnEEUTEV63",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "print(\"TFP Version\", tfp.__version__)\n",
        "print(\"TF  Version\",tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LbagonrStPb8"
      },
      "source": [
        "### Simulate the fishlike data\n",
        "\n",
        "In the next few cells you will simulate some (x,y)-data where the y increases on average linerly with x but has non-constant variance that so that scatterplot looks like a fish.You will fist simulate random distributed noise with non constant variance, then uniformly distributed x values between -1 and 6 and finally calculate corresponding y values with y = 2.7*x+noise (linear slope of 2.7 and intercept of 0). The variance of the noise will change, it starts with a high value of 12 and gets smaller until it is reaches a constant value of 1, then it grows again until a value of 15 to stay constant for a while and to decrease to 1 again. Look at the plot to understand the behavior of the variance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EGyWMIv6-PlO",
        "colab": {}
      },
      "source": [
        "### define variance structure of the simulation\n",
        "x1=np.arange(1,12,0.1)\n",
        "x1=x1[::-1]\n",
        "x2=np.repeat(1,30)\n",
        "x3=np.arange(1,15,0.1)\n",
        "x4=np.repeat(15,50)\n",
        "x5=x3[::-1]\n",
        "x6=np.repeat(1,20)\n",
        "x=np.concatenate([x1,x2,x3,x4,x5,x6])\n",
        "plt.plot(x)\n",
        "plt.xlabel(\"index\",size=16)\n",
        "plt.ylabel(\"sigma\",size=16)#pred\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N_wa7qrE_C-H"
      },
      "source": [
        "Now you will sample uniformly distributed x values in the range from -1 to 6. You will sample less x values in the range from -1 to 1. Finally you sort the x values (for ploting reasons). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60b0dOUd9wKL",
        "colab": {}
      },
      "source": [
        "# generation the x values for the simulated data\n",
        "np.random.seed(4710)\n",
        "noise=np.random.normal(0,x,len(x))\n",
        "np.random.seed(99)\n",
        "first_part=len(x1)\n",
        "x11=np.random.uniform(-1,1,first_part)\n",
        "np.random.seed(97)\n",
        "x12=np.random.uniform(1,6,len(noise)-first_part)\n",
        "x=np.concatenate([x11,x12])\n",
        "x=np.sort(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g81DrgF8CkuA"
      },
      "source": [
        "Let's put it all together to make the simulated fishlike data complete. You calculate y from the x values and the noise with a linear function where the slope is 2.7 and the intercept is 0, y=2.7*x+noise.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XDGPV4WX9wHU",
        "colab": {}
      },
      "source": [
        "## generation the y values for the simulated noise and the x values\n",
        "y=2.7*x+noise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rOZ-tdQ8YiQV",
        "colab": {}
      },
      "source": [
        "y=y.reshape((len(y),1))\n",
        "x=x.reshape((len(x),1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x2qConCfI1yQ",
        "colab": {}
      },
      "source": [
        "# lets visualize the data\n",
        "plt.scatter(x,y,color=\"steelblue\") \n",
        "plt.xlabel(\"x\",size=16)\n",
        "plt.ylabel(\"y\",size=16)#pred\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pA9H9VStAzow"
      },
      "source": [
        "#### Split data in train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_U-vd1VSFLBt"
      },
      "source": [
        "In the next cells you will spilt the data x and y into a training, validation and test set. To get a first train and test dataset you just randomly sample 25% of the x and y values in the test dataset and the rest is the training dataset. The resulting training dataset gets splitted again into a training and validation dataset (80% training and 20% validation). After the splitting of the dataset you need to make sure that all the x values  from every dataset are in increasing order for ploting reasons (note that you also need to reorder the corresponding y values for all datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNUefcVZqqg2",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=47)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=22)\n",
        "\n",
        "print(\"nr of traning samples = \",len(x_train))\n",
        "print(\"nr of validation samples = \",len(x_val))\n",
        "print(\"nr of test samples = \",len(x_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f-F99DjfxcAw",
        "colab": {}
      },
      "source": [
        "## reordering so x values are in increasiong order\n",
        "order_idx_train=np.squeeze(x_train.argsort(axis=0))\n",
        "x_train=x_train[order_idx_train]\n",
        "y_train=y_train[order_idx_train]\n",
        "\n",
        "order_idx_val=np.squeeze(x_val.argsort(axis=0))\n",
        "x_val=x_val[order_idx_val]\n",
        "y_val=y_val[order_idx_val]\n",
        "\n",
        "order_idx_test=np.squeeze(x_test.argsort(axis=0))\n",
        "x_test=x_test[order_idx_test]\n",
        "y_test=y_test[order_idx_test]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfKxdFYBH93m"
      },
      "source": [
        "Let's plot the training and validation data. You can see that it really looks a bit like a fish. In the following cells you will train different models on the training data, validate the loss (NLL) on the validation data and in the end you will predict the testdata with the best model. It's important to keep the testdata in a locked safe, because in practice it is unknown, until you decide which model you want to use to make a prediction. That is the reason why you will plot it only in the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aa9_Dwo5Nf-t",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(x_train,y_train,color=\"steelblue\")\n",
        "plt.xlabel(\"x\",size=16)\n",
        "plt.ylabel(\"y\",size=16)\n",
        "plt.title(\"train data\")\n",
        "plt.xlim([-1.5,6.5])\n",
        "plt.ylim([-30,55])\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(x_val,y_val,facecolors='none', edgecolors=\"steelblue\")\n",
        "plt.xlabel(\"x\",size=16)\n",
        "plt.ylabel(\"y\",size=16)\n",
        "plt.title(\"validation data\")\n",
        "plt.xlim([-1.5,6.5])\n",
        "plt.ylim([-30,55])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "13VLA4m_EV-D",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib_fU9tcRz_X",
        "colab_type": "text"
      },
      "source": [
        "## Fit a linear regression model with constant variance\n",
        "In the next cells you will define and fit a linear model on the simulated fish data with keras. You define a simple linear regression NN with only two parameters to model the output as a gaussian conditional probability distribution , for the loss we use the mse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Q3-VYLRyee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = Input(shape=(1,))\n",
        "params = Dense(1)(inputs) \n",
        "\n",
        "model_ = Model(inputs=inputs, outputs=params)\n",
        "model_.compile(Adam(learning_rate=0.1), loss=\"mse\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5RDEDFNRybC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8KHL9TiSdDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model_.fit(x_train, y_train, epochs=8000, verbose=0, validation_data=(x_val,y_val),batch_size=len(x_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg1NszifTmzE",
        "colab_type": "text"
      },
      "source": [
        "To calculate the  minimal NLL we need to know the optimal constant sigma that minimizes the NLL. To get the optimal sigma we use the formulat for the optimal signma and compute a slightly adjusted standart deviation of the residuals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll0SobKoShwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_train = model_.predict(x_train)\n",
        "preds_val = model_.predict(x_val)\n",
        "\n",
        "SSR= np.sum(np.square(y_train-preds_train))\n",
        "sigma_=np.sqrt((SSR)/(len(x_train)))\n",
        "sigma_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhKOx7r6S9Kz",
        "colab_type": "text"
      },
      "source": [
        "To calculate the NLL with the optimal constant sigma, you can use the formula for the $$NLL = \\frac{1}{n}\\sum_{i=1}^{n}- log(\\frac{1}{\\sqrt{2 \\pi \\sigma^2_x}})+\\frac{(y_i - \\mu_i)^2}{2 \\sigma^2_x}$$ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az1VDhsWShtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss with the estimated sigma\n",
        "NLL_train=np.mean(-np.log(1/(np.sqrt(2*np.pi*np.square(sigma_))))+\n",
        "                  ((np.square(y_train-model_.predict(x_train))/(2*np.square(sigma_)))))\n",
        "print(NLL_train)\n",
        "NLL_val=np.mean(-np.log(1/(np.sqrt(2*np.pi*np.square(sigma_))))+\n",
        "                ((np.square(y_val-model_.predict(x_val))/(2*np.square(sigma_)))))\n",
        "print(NLL_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dILQYvQ5UOpY",
        "colab_type": "text"
      },
      "source": [
        "Now you will plot the trained model with the resulting mean and +-2 sigma at each x value and see how well it fits the data. Remember in this model you used a linear model for the mean of the CPD and just fixed the standart deviation sigma to a constant value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USmwR7bwUN36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "x_pred = np.arange(-1,6,0.1)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(x_train,y_train,color=\"steelblue\") #observerd \n",
        "preds = model_.predict(x_pred)\n",
        "sigma = sigma_\n",
        "plt.plot(x_pred,preds,color=\"black\",linewidth=2)\n",
        "plt.plot(x_pred,preds+2*sigma,color=\"red\",linestyle=\"--\",linewidth=2) \n",
        "plt.plot(x_pred,preds-2*sigma,color=\"red\",linestyle=\"--\",linewidth=2)\n",
        "plt.xlabel(\"x\",size=16)\n",
        "plt.ylabel(\"y\",size=16)\n",
        "plt.title(\"train data\")\n",
        "plt.xlim([-1.5,6.5])\n",
        "plt.ylim([-30,55])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(x_val,y_val,color=\"steelblue\") #observerd \n",
        "plt.plot(x_pred,preds,color=\"black\",linewidth=2)\n",
        "plt.plot(x_pred,preds+2*sigma,color=\"red\",linestyle=\"--\",linewidth=2) \n",
        "plt.plot(x_pred,preds-2*sigma,color=\"red\",linestyle=\"--\",linewidth=2)\n",
        "plt.xlabel(\"x\",size=16)\n",
        "plt.ylabel(\"y\",size=16)\n",
        "plt.title(\"validation data\")\n",
        "plt.xlim([-1.5,6.5])\n",
        "plt.ylim([-30,55])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJgUkCIeo4y1"
      },
      "source": [
        "## Fit a linear regression model with constant variance in TFP\n",
        "\n",
        "Let's try to model the standart deviation sigma in a constant way and use the TFP Framework. For this we need to use a little trick. To define a constant standart deviation, you can use a Normal distribution in TFP and you fit both parameters the mean and sigma. You don't use any hidden layers in between. As loss function we use the NLL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V32TXJMHYBO5",
        "colab": {}
      },
      "source": [
        "def NLL(y, distr):\n",
        "  return -distr.log_prob(y) #A\n",
        "\n",
        "def my_dist(params): \n",
        "  return tfd.Normal(loc=params[:,0:1], scale=1e-3 + tf.math.softplus(0.05 * params[:,1:2]))# both parameters are learnable #C #D\n",
        "\n",
        "input1 = Input(shape=(1,))\n",
        "input2 = Input(shape=(1,))\n",
        "out1 = Dense(1)(input1) #B\n",
        "out2 = Dense(1,use_bias=False)(input2) #B\n",
        "params = Concatenate()([out1,out2]) #C\n",
        "dist = tfp.layers.DistributionLambda(my_dist)(params) #C #D\n",
        "\n",
        "model_const_sd = Model(inputs=[input1,input2], outputs=dist) ## use a trick with two inputs, input2 is just ones\n",
        "model_const_sd.compile(tf.keras.optimizers.Adam(1), loss=NLL) \n",
        "\n",
        "#A Define NLL of the model \n",
        "#B Setting up the NN with two output node\n",
        "#C The first output node defines the mean (loc)\n",
        "#D The second output defines the standard deviation (scale) via the softplus function "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ng9q8paln6i9",
        "colab": {}
      },
      "source": [
        "model_const_sd.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iRn1dI5Kn8n7",
        "colab": {}
      },
      "source": [
        "history = model_const_sd.fit([x_train,np.expand_dims(np.ones(len(x_train)),1)], y_train, batch_size=len(x_train), epochs=8000, verbose=0, \n",
        "                                validation_data=([x_val,np.expand_dims(np.ones(len(x_val)),1)],y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sPkN7OHU_-uy"
      },
      "source": [
        "In the next cell you define two models to predict the mean $\\mu$ and the standart deviation $\\sigma$ of the output distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q8s5U5baSvPl",
        "colab": {}
      },
      "source": [
        "model_const_sd_mean = Model(inputs=[input1,input2], outputs=dist.mean())\n",
        "model_const_sd_sd = Model(inputs=[input1,input2], outputs=dist.stddev())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVh77gc3ImTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_const_sd_sd.predict([x_train,np.expand_dims(np.ones(len(x_train)),1)])[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IetFvGu_WQc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigma_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iyM1HouzSvPp",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylabel('NLL')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhuzHofA30Hk",
        "colab_type": "text"
      },
      "source": [
        "#### Result:  constant sigma TFP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BIY1kiyDSvPx",
        "colab": {}
      },
      "source": [
        "print(model_const_sd.evaluate([x_train,np.expand_dims(np.ones(len(x_train)),1)],y_train, verbose=0))\n",
        "print(model_const_sd.evaluate([x_val,np.expand_dims(np.ones(len(x_val)),1)],y_val, verbose=0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR6eYfQvYp4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss with the estimated sigma\n",
        "NLL_train=np.mean(-np.log(1/(np.sqrt(2*np.pi*np.square(sigma_))))+\n",
        "                  ((np.square(y_train-model_.predict(x_train))/(2*np.square(sigma_)))))\n",
        "print(NLL_train)\n",
        "NLL_val=np.mean(-np.log(1/(np.sqrt(2*np.pi*np.square(sigma_))))+\n",
        "                ((np.square(y_val-model_.predict(x_val))/(2*np.square(sigma_)))))\n",
        "print(NLL_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vgpvOOa_SvP5",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "x_pred = np.arange(-1,6,0.1)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(x_train,y_train,color=\"steelblue\") #observerd \n",
        "preds = model_const_sd_mean.predict([x_pred,np.expand_dims(np.ones(len(x_pred)),1)])\n",
        "plt.plot(x_pred,preds,color=\"black\",linewidth=2)\n",
        "plt.plot(x_pred,preds+2*model_const_sd_sd.predict([x_pred,np.expand_dims(np.ones(len(x_pred)),1)]),color=\"red\",linestyle=\"--\",linewidth=2) \n",
        "plt.plot(x_pred,preds-2*model_const_sd_sd.predict([x_pred,np.expand_dims(np.ones(len(x_pred)),1)]),color=\"red\",linestyle=\"--\",linewidth=2)\n",
        "plt.xlabel(\"x\",size=16)\n",
        "plt.ylabel(\"y\",size=16)\n",
        "plt.title(\"train data\")\n",
        "plt.xlim([-1.5,6.5])\n",
        "plt.ylim([-30,55])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(x_val,y_val,color=\"steelblue\") #observerd \n",
        "plt.plot(x_pred,preds,color=\"black\",linewidth=2)\n",
        "plt.plot(x_pred,preds+2*model_const_sd_sd.predict([x_pred,np.expand_dims(np.ones(len(x_pred)),1)]),color=\"red\",linestyle=\"--\",linewidth=2) \n",
        "plt.plot(x_pred,preds-2*model_const_sd_sd.predict([x_pred,np.expand_dims(np.ones(len(x_pred)),1)]),color=\"red\",linestyle=\"--\",linewidth=2)\n",
        "plt.xlabel(\"x\",size=16)\n",
        "plt.ylabel(\"y\",size=16)\n",
        "plt.title(\"validation data\")\n",
        "plt.xlim([-1.5,6.5])\n",
        "plt.ylim([-30,55])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BZvs9osI_tIR"
      },
      "source": [
        "Yout see that the optimal sigma in both models is the same with very small differences, the NLL loss for both models is also practically the same in the train dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sxTLAx5a-6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}