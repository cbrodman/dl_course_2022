{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19_bayesian_coin_toss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TdjHPdOouzBM"
      },
      "source": [
        "# A Bayesian model for a coin toss\n",
        "\n",
        "**Goal:** In this notebook you will learn how you can calculate the postirior distribution for the parameter $\\theta$ as well as the predictvie distribution for the outcome of a bernoulli experiment. You will use the brute force approach and pick different prior distributions for the parameter $\\theta$.\n",
        "Note that you will use discrete values for the prior and postirior of $\\theta$ here. You will work with sums in this notebook to approximate the integrals.\n",
        "\n",
        "\n",
        "**Dataset:** You work with the observed values of coin tosses, 1 for head and 0 for tail\n",
        "\n",
        "**Content:**\n",
        "* Define a uniform prior for $\\theta$\n",
        "* Evaluate the joined likelihood and the unnormalized posterior at one specific $\\theta$\n",
        "* Calculate the joined likelihood, the unnormalized posterior and the normalized posterior for a range of  $\\theta$\n",
        "* Calculate and plot the prior predictive distribution and the posterior predictive distribution\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2bB8noh1s84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdYuC4yRP0ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow_probability==0.8.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4etcsdnkuTpU"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_VSparStMHS6",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "plt.style.use('default') \n",
        "%matplotlib inline\n",
        "tfd = tfp.distributions\n",
        "print(\"TFP Version\", tfp.__version__)\n",
        "print(\"TF  Version\",tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dKWgqZJsuir7"
      },
      "source": [
        "#### Create train data\n",
        "In the next cell we set the outcome of three coin tosses to one, meaning we get three times head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KYNnP9ZzR9wq",
        "colab": {}
      },
      "source": [
        "obs_data=np.repeat(1,3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kwCJtfU9vPGu"
      },
      "source": [
        "## Maximum likelihood approach to fit the Bernoulli model\n",
        "Because we have a binary outcome (heads or tails) we need a Bernoulli model to describe the data. A Bernoulli model has only one parameter $\\theta$ which is the probability to get outcome one (head). In the cell below you use the maximum liklihood approach to estimate the parameter $\\theta$ and the standart deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LjPTrOWPBboz",
        "colab": {}
      },
      "source": [
        "### ML\n",
        "est_theta_ml=np.mean(obs_data)\n",
        "print(\"ML theta\",est_theta_ml)\n",
        "sd_est_theta_ml = est_theta_ml * (1. - est_theta_ml)\n",
        "print(\"sd theta\",sd_est_theta_ml)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HfYA2KnwxTvu"
      },
      "source": [
        "## Bayes approach to fit the Bernoulli model\n",
        "For the Bayes approach we fist need to define a prior distribution for the patameter $\\theta$ of the Bernoulli distribution. We evaluate the distributions at discrete points in the range 0.05 to 0.95. You use a uniform prior where every theta has the same probability. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gv8JQ7ph-xtM",
        "colab": {}
      },
      "source": [
        "theta=np.arange(0.05,1,0.05)\n",
        "print(theta)\n",
        "prior = 1/len(theta) #The normalizing constant of the prior"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o5eAD6uJyPDM"
      },
      "source": [
        "Let's evaluate the joined likelihood and the unnormalized posterior at one specific $\\theta = 0.5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cxNnVecuxqC2",
        "colab": {}
      },
      "source": [
        "dist = tfp.distributions.Bernoulli(probs=0.5) #one specific theta\n",
        "print(np.prod(dist.prob(obs_data))) #joint likelihood\n",
        "print(np.prod(dist.prob(obs_data))*prior) #unnormalized posterior"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9z9qObHIyy3S"
      },
      "source": [
        "Repeating the steps above for all thetas from the range of 0.05 until 0.95."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_PGX3RSbF9qT",
        "colab": {}
      },
      "source": [
        "res = np.zeros((len(theta),5))\n",
        "for i in range(0,len(theta)):\n",
        "  dist = tfp.distributions.Bernoulli(probs=theta[i])   \n",
        "  res[i,0:4]=np.array((theta[i],np.prod(dist.prob(obs_data)),prior,np.prod(dist.prob(obs_data))*prior))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xtOv-ccHzVtI"
      },
      "source": [
        "Note that we need to normalize the posterior, so that it sums up to one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i1EhebTRHu-G",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "res=pd.DataFrame(res,columns=[\"theta\",\"jointlik\",\"prior\",\"unnorm_post\",\"post\"])\n",
        "res[\"post\"]=res[\"unnorm_post\"]/np.sum(res[\"unnorm_post\"])\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7qRHkk9Qznam"
      },
      "source": [
        "### Posterior and prior for $\\theta$\n",
        "\n",
        "Let's plot the prior and the posterior for $\\theta$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7z6X_zLbO6YJ",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.stem(res[\"theta\"],res[\"prior\"],use_line_collection=True)\n",
        "plt.xlabel(\"theta\")\n",
        "plt.ylabel(\"probability\")\n",
        "plt.ylim([0,0.5])\n",
        "plt.title(\"prior distribution\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.stem(res[\"theta\"],res[\"post\"],use_line_collection=True)\n",
        "plt.ylim([0,0.5])\n",
        "plt.xlabel(\"theta\")\n",
        "plt.ylabel(\"probability\")\n",
        "plt.title(\"posterior distribution\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6qwjotjbfUSq"
      },
      "source": [
        "### Posterior and prior for head\n",
        "\n",
        "The predictive distribution is given by\n",
        "\n",
        "$\n",
        "p(y|D)=\\sum_i p(y|\\theta_i)p(\\theta_i|D)\n",
        "$\n",
        "\n",
        "For the probability of head, we have $\\theta_i = p(y=1|\\theta_i)$. For $p(\\theta_i|D)$, we first use the posterior distribution resulting in\n",
        "\n",
        "$\n",
        "  p(y=1|D) = \\sum_i \\theta_i p(\\theta_i|D)\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2qpa4ewsiGHL",
        "colab": {}
      },
      "source": [
        "py1_post = np.sum((res[\"theta\"])*res[\"post\"])\n",
        "py0_post = 1.0 - py1_post\n",
        "py0_post, py1_post"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yc7754pHiWxE"
      },
      "source": [
        "Similar, you get for the prior distributions: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PrL-DoWhhiuC",
        "colab": {}
      },
      "source": [
        "py1_prior = np.sum((res[\"theta\"])*res[\"prior\"]) \n",
        "py0_prior = 1 - py1_prior\n",
        "py0_prior, py1_prior"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KVhF4bDJiwlT"
      },
      "source": [
        "We plot this togther with the figure above.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T79DCZEOuwb4",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(16,12))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.stem(res[\"theta\"],res[\"prior\"],use_line_collection=True)\n",
        "plt.xlabel(\"theta\")\n",
        "plt.ylabel(\"probability\")\n",
        "plt.ylim([0,0.5])\n",
        "plt.title(\"prior distribution for theta\")\n",
        "plt.subplot(2,2,2)\n",
        "plt.stem(res[\"theta\"],res[\"post\"],use_line_collection=True)\n",
        "plt.ylim([0,0.5])\n",
        "plt.xlabel(\"theta\")\n",
        "plt.ylabel(\"probability\")\n",
        "plt.title(\"posterior distribution for theta\")\n",
        "plt.savefig(\"test.pdf\")\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.stem([0,1],[py0_prior,py1_prior],use_line_collection=True)\n",
        "plt.xlabel(\"Y\")\n",
        "plt.ylabel(\"P(y)\")\n",
        "plt.ylim([0,1])\n",
        "plt.title(\"prior predictive distribution\")\n",
        "plt.subplot(2,2,4)\n",
        "plt.stem([0,1],[py0_post,py1_post],use_line_collection=True)\n",
        "plt.ylim([0,1])\n",
        "plt.xlabel(\"Y\")\n",
        "plt.ylabel(\"P(y)\")\n",
        "plt.title(\"posterior predictive distribution\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6O-XLU9BYy7T"
      },
      "source": [
        "### Posterior and the predictive distribution for different observed data\n",
        "\n",
        "*Exercise: Let's assume fist you observed 40 times head and then you observed 11 times head and 9 times tail. How does the posterior and the predictive distribution look, for these two cases?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dUrZY3abdBPz",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yL6K_2GlpO8W"
      },
      "source": [
        "### Different prior\n",
        "\n",
        "*Exercise 2: Let's now repeat the experiment with a non-uniform distributed prior. You will use a halfcircle as prior. With this prior calculate again the posterior when you fist observe 40 times head and then you observed 11 times head and 9 times tail. How does the posterior and the predictive distribution look with the new prior?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T-lnHLIPc6yp",
        "colab": {}
      },
      "source": [
        "prior=np.sqrt(np.square(0.5)-np.square(theta-0.5))-0.2\n",
        "prior=prior/np.sum(prior)#normalzation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eX5soBHCq46R",
        "colab": {}
      },
      "source": [
        "plt.stem(theta,prior,use_line_collection=True)\n",
        "plt.xlim([0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmxy5_n77jbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}